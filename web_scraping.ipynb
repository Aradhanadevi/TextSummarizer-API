{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install beautifulsoup4 requests pandas\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL of Hindustan Times\n",
    "BASE_URL = \"https://www.hindustantimes.com\"\n",
    "\n",
    "# Function to get article links from a section page\n",
    "def get_article_links(section_url):\n",
    "    response = requests.get(section_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {section_url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extracting article links\n",
    "    links = [BASE_URL + a[\"href\"] for a in soup.find_all(\"a\", href=True) if \"/india-news/\" in a[\"href\"]]\n",
    "    \n",
    "    return list(set(links))  # Remove duplicates\n",
    "\n",
    "# Function to scrape a single article\n",
    "def scrape_article(url):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch article: {url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title\"\n",
    "    \n",
    "    # Extract text content\n",
    "    article_text = \" \".join([p.text.strip() for p in soup.find_all(\"p\")])\n",
    "\n",
    "    return {\"Title\": title, \"URL\": url, \"Article\": article_text}\n",
    "\n",
    "# Define section to scrape\n",
    "section_url = \"https://www.hindustantimes.com/india-news\"\n",
    "article_links = get_article_links(section_url)\n",
    "\n",
    "# Scrape multiple articles\n",
    "data = []\n",
    "for i, link in enumerate(article_links[:10]):  # Limit to first 10 articles\n",
    "    print(f\"Scraping {i+1}/{len(article_links)}: {link}\")\n",
    "    article_data = scrape_article(link)\n",
    "    if article_data:\n",
    "        data.append(article_data)\n",
    "    time.sleep(2)  # Delay to prevent blocking\n",
    "\n",
    "# Save scraped data to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"hindustan_times_articles.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Scraping complete! Data saved as hindustan_times_articles.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
